Datasets:

https://huggingface.co/datasets/okite97/news-data
https://huggingface.co/datasets/heegyu/news-category-dataset
https://huggingface.co/datasets/AndyReas/frontpage-news
https://huggingface.co/datasets/Deysi/spam-detection-dataset
https://huggingface.co/datasets/mshenoda/spam-messages
https://huggingface.co/datasets/cardiffnlp/tweet_eval



import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import umap

# ------------------------
# YOUR EMBEDDING FUNCTION
# ------------------------
def get_embedding(text: str):
    """
    Replace this with your real embedding API call.
    """
    # Example: return HF / OpenAI / MiniLM embedding
    raise NotImplementedError


# ------------------------
# LOAD YOUR DATA
# df must have: columns = ["text", "label"]
# label ‚àà {"promotion", "non-promotion"}
# ------------------------
df = pd.read_csv("data.csv")   # update path

# ------------------------
# COMPUTE EMBEDDINGS
# ------------------------
embeddings = []

for txt in tqdm(df["text"], desc="Embedding"):
    emb = get_embedding(txt)
    emb = np.array(emb)
    embeddings.append(emb)

embeddings = np.vstack(embeddings)

# ------------------------
# DIMENSIONALITY REDUCTION
# Choose one:
#   1. UMAP (best global structure)
#   2. t-SNE (best cluster separation)
#   3. PCA (fast baseline)
# ------------------------

# ---- Option A: UMAP ----
reducer = umap.UMAP(
    n_neighbors=20,
    min_dist=0.1,
    metric='cosine',
    random_state=42
)
points_2d = reducer.fit_transform(embeddings)

# ---- Option B: t-SNE ----
# points_2d = TSNE(
#     n_components=2,
#     perplexity=30,
#     metric='cosine',
#     random_state=42
# ).fit_transform(embeddings)

# ---- Option C: PCA ----
# points_2d = PCA(n_components=2).fit_transform(embeddings)

# ------------------------
# PLOT
# ------------------------
plt.figure(figsize=(10, 8))
colors = df["label"].map({"promotion": "red", "non-promotion": "blue"})

plt.scatter(
    points_2d[:, 0],
    points_2d[:, 1],
    c=colors,
    alpha=0.6,
    s=40
)

plt.title("Promotion vs Non-Promotion Embedding Space")
plt.xlabel("Dim 1")
plt.ylabel("Dim 2")
plt.grid(True)
plt.show()







Below is **clean, ready-to-use code** to show:

### **1. Plot ‚Üí Each domain separately**

### **2. Plot ‚Üí Within each domain, topic-wise scatter**

This will tell you **exactly which domain and which topic** causes promotion vs non-promotion overlap.

---

# ‚úÖ **Assumptions**

Your DF contains:

```
df["text"]
df["label"]           # promotion / non-promotion
df["domain"]          # e.g. Finance, Shopping, Travel
df["topic"]           # e.g. Credit card offers, Transaction confirmation
embeddings_2d         # your UMAP / TSNE output
```

---

# ‚úÖ **Step-1: Plot DOMAIN-wise**

```python
unique_domains = df["domain"].unique()

for dom in unique_domains:
    mask = df["domain"] == dom
    pts = points_2d[mask]
    labels = df.loc[mask, "label"]
    
    plt.figure(figsize=(10, 8))
    colors = labels.map({"promotion": "red", "non-promotion": "blue"})

    plt.scatter(
        pts[:, 0],
        pts[:, 1],
        c=colors,
        alpha=0.7,
        s=45
    )

    plt.title(f"Embedding Space ‚Äî Domain: {dom}")
    plt.xlabel("Dim 1")
    plt.ylabel("Dim 2")
    plt.grid(True)
    plt.show()
```

This tells you **which domain clusters cleanly and which overlaps**.

---

# ‚úÖ **Step-2: Plot TOPIC-wise (inside each domain)**

Run this to see overlap **per topic per domain**:

```python
unique_domains = df["domain"].unique()

for dom in unique_domains:
    df_dom = df[df["domain"] == dom]
    pts_dom = points_2d[df["domain"] == dom]

    unique_topics = df_dom["topic"].unique()

    for topic in unique_topics:
        mask = (df["domain"] == dom) & (df["topic"] == topic)
        pts = points_2d[mask]
        labels = df.loc[mask, "label"]
        
        plt.figure(figsize=(10, 8))
        colors = labels.map({"promotion": "red", "non-promotion": "blue"})

        plt.scatter(
            pts[:, 0],
            pts[:, 1],
            c=colors,
            alpha=0.7,
            s=45
        )

        plt.title(f"Domain: {dom} ‚Äî Topic: {topic}")
        plt.xlabel("Dim 1")
        plt.ylabel("Dim 2")
        plt.grid(True)
        plt.show()
```

This gives you **one plot per topic** inside each domain.
You will instantly see:

‚úì Topics that cluster well
‚úì Topics that create confusion (red-blue mixing)
‚úì Domains where promotional language overlaps with informational messaging

---

# üëâ **Optional (very useful): Side-by-side topic plots for each domain**

If you want to view all topics **in one grid per domain**, I can generate that too.

Just say: **‚Äúgive me grid plot version‚Äù**.

