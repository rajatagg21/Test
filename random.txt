metrics=[
    tf.keras.metrics.Precision(name="precision"),
    tf.keras.metrics.Recall(name="recall"),
    tf.keras.metrics.Accuracy(name="accuracy"),
    tf.keras.metrics.F1Score(name="f1-score")
]

import matplotlib.pyplot as plt

def save_loss_curve(history, output_path):
    """
    Saves train/val loss curve to PNG without displaying it.
    """
    plt.figure(figsize=(8, 5))
    plt.plot(history.history["loss"], label="Train Loss")
    plt.plot(history.history["val_loss"], label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss")
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()   # <-- important: prevents display and frees memory


def transformer_block(x, num_heads=4, ff_dim=256, dropout=0.1):
    # Multi-head self-attention
    attn_output = layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=x.shape[-1]
    )(x, x)

    # Skip connection + LayerNorm
    out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)

    # Feed Forward Network
    ffn = tf.keras.Sequential([
        layers.Dense(ff_dim, activation="relu"),
        layers.Dense(x.shape[-1])
    ])

    ffn_output = ffn(out1)
    ffn_output = layers.Dropout(dropout)(ffn_output)

    # Second skip + layernorm
    return layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)


def build_model(strategy):
    with strategy.scope():
        inp = layers.Input(shape=(1,), dtype=tf.string)

        # RETVec outputs [batch, seq_len, 256]
        x = RETVecTokenizer(model="retvec-v1")(inp)

        # Transformer Block 1
        x = transformer_block(x, num_heads=4, ff_dim=256, dropout=0.1)

        # Transformer Block 2 (recommended)
        x = transformer_block(x, num_heads=4, ff_dim=256, dropout=0.1)

        # Global pooling
        x = layers.GlobalAveragePooling1D()(x)

        # Dense Layers
        x = layers.Dense(256, activation="relu")(x)
        x = layers.Dropout(0.3)(x)

        x = layers.Dense(128, activation="relu")(x)
        x = layers.Dropout(0.2)(x)

        out = layers.Dense(1, activation="sigmoid")(x)

        model = tf.keras.Model(inp, out)

        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
            loss="binary_crossentropy",
            metrics=["accuracy"],
        )

    model.summary(print_fn=lambda s: logging.info(s))
    return model




callbacks=[
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=1,
        min_lr=1e-6
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )
]

model.fit(train_ds, validation_data=val_ds, epochs=args.epochs, callbacks=callbacks)




def load_dataset(path, batch_size, shuffle=True):
    df = pd.read_csv(path)

    texts = df["text"].astype(str).tolist()
    labels = df["label"].astype(int).tolist()

    ds = tf.data.Dataset.from_tensor_slices((texts, labels))

    if shuffle:
        ds = ds.shuffle(buffer_size=len(texts), reshuffle_each_iteration=True)

    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    return ds


# GRU + self attention block

from tensorflow.keras import layers, models
from keras_nlp.layers import RETVecTokenizer

def build_model(strategy):
    with strategy.scope():

        inputs = layers.Input(shape=(1,), dtype=tf.string)
        
        # RETVec encodes text â†’ tensor (batch, seq_len, 256)
        x = RETVecTokenizer(model="retvec-v1")(inputs)

        # BiGRU stack
        x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(x)
        x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)

        # Inbuilt MultiHeadAttention (self-attention)
        attn_output = layers.MultiHeadAttention(
            num_heads=4, 
            key_dim=64,
            dropout=0.1
        )(x, x)

        # Residual + LayerNorm (make it transformer-style)
        x = layers.LayerNormalization()(x + attn_output)

        # Global pooling to get sentence-level embedding
        x = layers.GlobalAveragePooling1D()(x)

        # Dense layers
        x = layers.Dense(128, activation="relu")(x)
        x = layers.Dropout(0.2)(x)

        outputs = layers.Dense(1, activation="sigmoid")(x)

        model = models.Model(inputs, outputs)

        model.compile(
            optimizer=tf.keras.optimizers.Adam(1e-3),
            loss="binary_crossentropy",
            metrics=["accuracy"]
        )

    model.summary()
    return model


