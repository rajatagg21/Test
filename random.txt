import tensorflow as tf
from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np


class PRF1Callback(tf.keras.callbacks.Callback):
    def __init__(self, val_data, logger=None):
        super().__init__()
        self.val_data = val_data
        self.logger = logger

    def on_epoch_end(self, epoch, logs=None):
        val_X, val_y = self.val_data

        # Forward pass
        preds = self.model.predict(val_X, verbose=0)

        # Binary classification
        if preds.shape[-1] == 1:
            y_pred = (preds.ravel() > 0.5).astype(int)
            y_true = val_y.astype(int)
        else:
            # Multi-class
            y_pred = np.argmax(preds, axis=-1)
            y_true = np.argmax(val_y, axis=-1)

        # Compute metrics
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)

        # Add to logs dict for model.fit() purposes
        logs = logs or {}
        logs["val_precision"] = precision
        logs["val_recall"] = recall
        logs["val_f1"] = f1

        msg = (f"Epoch {epoch+1}: "
               f"Precision={precision:.4f}, "
               f"Recall={recall:.4f}, "
               f"F1={f1:.4f}")

        # Print
        print("\n" + msg)

        # Log to logger instance (if provided)
        if self.logger is not None:
            self.logger.info(msg)


prf1_cb = PRF1Callback(
    val_data=(val_X, val_y),
    logger=logger
)

model.fit(
    train_dataset,
    validation_data=(val_X, val_y),
    epochs=5,
    callbacks=[prf1_cb]
)



import matplotlib.pyplot as plt

def save_loss_curve(history, output_path):
    """
    Saves train/val loss curve to PNG without displaying it.
    """
    plt.figure(figsize=(8, 5))
    plt.plot(history.history["loss"], label="Train Loss")
    plt.plot(history.history["val_loss"], label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Train vs Validation Loss")
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()   # <-- important: prevents display and frees memory


def transformer_block(x, num_heads=4, ff_dim=256, dropout=0.1):
    # Multi-head self-attention
    attn_output = layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=x.shape[-1]
    )(x, x)

    # Skip connection + LayerNorm
    out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)

    # Feed Forward Network
    ffn = tf.keras.Sequential([
        layers.Dense(ff_dim, activation="relu"),
        layers.Dense(x.shape[-1])
    ])

    ffn_output = ffn(out1)
    ffn_output = layers.Dropout(dropout)(ffn_output)

    # Second skip + layernorm
    return layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)


def build_model(strategy):
    with strategy.scope():
        inp = layers.Input(shape=(1,), dtype=tf.string)

        # RETVec outputs [batch, seq_len, 256]
        x = RETVecTokenizer(model="retvec-v1")(inp)

        # Transformer Block 1
        x = transformer_block(x, num_heads=4, ff_dim=256, dropout=0.1)

        # Transformer Block 2 (recommended)
        x = transformer_block(x, num_heads=4, ff_dim=256, dropout=0.1)

        # Global pooling
        x = layers.GlobalAveragePooling1D()(x)

        # Dense Layers
        x = layers.Dense(256, activation="relu")(x)
        x = layers.Dropout(0.3)(x)

        x = layers.Dense(128, activation="relu")(x)
        x = layers.Dropout(0.2)(x)

        out = layers.Dense(1, activation="sigmoid")(x)

        model = tf.keras.Model(inp, out)

        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
            loss="binary_crossentropy",
            metrics=["accuracy"],
        )

    model.summary(print_fn=lambda s: logging.info(s))
    return model




callbacks=[
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=1,
        min_lr=1e-6
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )
]

model.fit(train_ds, validation_data=val_ds, epochs=args.epochs, callbacks=callbacks)




def load_dataset(path, batch_size, shuffle=True):
    df = pd.read_csv(path)

    texts = df["text"].astype(str).tolist()
    labels = df["label"].astype(int).tolist()

    ds = tf.data.Dataset.from_tensor_slices((texts, labels))

    if shuffle:
        ds = ds.shuffle(buffer_size=len(texts), reshuffle_each_iteration=True)

    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    return ds


# GRU + self attention block

from tensorflow.keras import layers, models
from keras_nlp.layers import RETVecTokenizer

def build_model(strategy):
    with strategy.scope():

        inputs = layers.Input(shape=(1,), dtype=tf.string)
        
        # RETVec encodes text â†’ tensor (batch, seq_len, 256)
        x = RETVecTokenizer(model="retvec-v1")(inputs)

        # BiGRU stack
        x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(x)
        x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)

        # Inbuilt MultiHeadAttention (self-attention)
        attn_output = layers.MultiHeadAttention(
            num_heads=4, 
            key_dim=64,
            dropout=0.1
        )(x, x)

        # Residual + LayerNorm (make it transformer-style)
        x = layers.LayerNormalization()(x + attn_output)

        # Global pooling to get sentence-level embedding
        x = layers.GlobalAveragePooling1D()(x)

        # Dense layers
        x = layers.Dense(128, activation="relu")(x)
        x = layers.Dropout(0.2)(x)

        outputs = layers.Dense(1, activation="sigmoid")(x)

        model = models.Model(inputs, outputs)

        model.compile(
            optimizer=tf.keras.optimizers.Adam(1e-3),
            loss="binary_crossentropy",
            metrics=["accuracy"]
        )

    model.summary()
    return model


