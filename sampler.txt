import random
from torch.utils.data import Sampler

class LangDomainBalancedSampler(Sampler):
    """
    Balanced sampler that:
      - Creates 1 balanced batch per (language, domain)
      - Each batch contains 50% positives, 50% negatives
      - Performs oversampling when needed
      - Works with HF Trainer
    """

    def __init__(self, dataset, batch_size):
        assert batch_size % 2 == 0, "Batch size must be even"
        self.dataset = dataset
        self.batch_size = batch_size
        self.half = batch_size // 2

        # Build groups for (language, domain, label)
        self.groups = self._build_groups()
        self.group_keys = list(self.groups.keys())

    def _build_groups(self):
        groups = {}
        for idx, item in enumerate(self.dataset):
            key = (item["language"], item["domain"])
            if key not in groups:
                groups[key] = {0: [], 1: []}
            groups[key][item["label"]].append(idx)
        return groups

    def __iter__(self):
        keys = self.group_keys.copy()
        random.shuffle(keys)

        for key in keys:
            pos_list = self.groups[key][1]
            neg_list = self.groups[key][0]

            # Cannot create a balanced batch for this group
            if len(pos_list) == 0 or len(neg_list) == 0:
                continue

            # ---- Positive sampling ----
            pos_indices = (
                random.sample(pos_list, self.half)
                if len(pos_list) >= self.half
                else random.choices(pos_list, k=self.half)
            )

            # ---- Negative sampling ----
            neg_indices = (
                random.sample(neg_list, self.half)
                if len(neg_list) >= self.half
                else random.choices(neg_list, k=self.half)
            )

            batch = pos_indices + neg_indices
            random.shuffle(batch)

            for idx in batch:
                yield idx  # Dataloader groups sequential indices into batches

    def __len__(self):
        valid_group_count = sum(
            1
            for k in self.group_keys
            if len(self.groups[k][0]) > 0 and len(self.groups[k][1]) > 0
        )
        return valid_group_count * self.batch_size



from transformers import Trainer
from torch.utils.data import DataLoader

class BalancedTrainer(Trainer):
    def get_train_dataloader(self):
        sampler = LangDomainBalancedSampler(
            self.train_dataset,
            batch_size=self.args.per_device_train_batch_size
        )

        return DataLoader(
            self.train_dataset,
            batch_size=self.args.per_device_train_batch_size,
            sampler=sampler,
            collate_fn=self.data_collator
        )

    # Eval â†’ do NOT use custom sampler
    # HF default behavior is correct


trainer = BalancedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

------------------------------------------------------

## class weights

from torch import nn
import torch
from transformers import AutoModel, SequenceClassifierOutput

class MiniLMWithHead(nn.Module):
    def __init__(self, model_name, num_labels=2, class_weights=None):
        super().__init__()

        self.encoder = AutoModel.from_pretrained(model_name)
        hidden_size = self.encoder.config.hidden_size

        # Freeze backbone if needed
        for p in self.encoder.parameters():
            p.requires_grad = False

        # classifier head
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_labels)
        )

        # register class weights
        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float)
        else:
            self.class_weights = None

        self.num_labels = num_labels
        self.loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=False,
            return_dict=True,
        )

        # CLS pooling
        cls = outputs.last_hidden_state[:, 0, :]   # [batch, hidden]

        logits = self.classifier(cls)

        loss = None
        if labels is not None:
            loss = self.loss_fn(logits, labels)

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
        )


from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import torch

labels = np.array(train_labels)
cls_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(labels),
    y=labels
)

model = MiniLMWithHead(
    model_name="nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large",
    num_labels=2,
    class_weights=cls_weights
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)
trainer.train()


---------------------------------------------
## classification head with transformer block

import torch
import torch.nn as nn
from transformers import AutoModel, SequenceClassifierOutput

class MiniLMClassifier(nn.Module):
    def __init__(self, model_name, num_labels=2):
        super().__init__()

        # --- Backbone ---
        self.encoder = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.encoder.config.hidden_size

        # Freeze MiniLM backbone
        for p in self.encoder.parameters():
            p.requires_grad = False

        # --- ALWAYS add 1 transformer block on top ---
        self.top_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=self.hidden_size,
                nhead=8,
                dim_feedforward=self.hidden_size * 4,
                batch_first=True
            ),
            num_layers=1
        )

        # --- Classifier head ---
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size * 4),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(self.hidden_size * 4, num_labels),
        )

        self.loss_fn = nn.CrossEntropyLoss()

    def mean_pooling(self, hidden_states, attention_mask):
        mask = attention_mask.unsqueeze(-1)                  # [B, L, 1]
        summed = (hidden_states * mask).sum(dim=1)           # [B, H]
        counts = mask.sum(dim=1)                             # [B, 1]
        return summed / counts

    def forward(self, input_ids, attention_mask, labels=None):
        # --- Get MiniLM outputs ---
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state            # [B, L, H]

        # --- Process through extra transformer block ---
        hidden_states = self.top_encoder(hidden_states)

        # --- Mean pooling ---
        pooled = self.mean_pooling(hidden_states, attention_mask)

        # --- Classifier ---
        logits = self.classifier(pooled)

        # --- Compute loss ---
        loss = None
        if labels is not None:
            loss = self.loss_fn(logits, labels)

        # --- HF output ---
        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
