PROMOTIONAL (Advertisement) – Opt-Out–Based Topics

These exist to market, influence, or re-engage, and therefore require opt-out.

Marketing_Broadcast_Messages: Mass promotional messages advertising products, services, or offers with STOP footer.

Promotional_Subscription_Campaigns: Ongoing marketing streams users opted into, with unsubscribe to stop promotions.

Brand_Reengagement_Alerts: Win-back, inactive-user, or loyalty nudges carrying opt-out text.

Offer_And_Discount_Alerts: Coupons, sales, cashback, limited-time deals with mandatory unsubscribe clause.

Promotional_Product_Announcements: New launches, feature promotions, cross-sell or upsell messages with STOP text.

ThirdParty_Promotional_Messages: Partner or sponsored promotions sent under shared consent with opt-out footer.

NON-PROMOTIONAL (Service / Transactional) – Opt-Out–Based Topics

These exist to fulfill a service obligation, but still include opt-out due to regulation, platform, or consent rules.

Consent_Governed_Service_Alerts: Service notifications sent under user consent that legally require opt-out language.

Transactional_With_Compliance_Footer: Order, payment, or booking updates carrying STOP text due to channel rules.

Subscription_Service_Notifications: Renewal, expiry, usage, or plan-change alerts with unsubscribe option.

Regulatory_Required_Communications: Mandatory notices (policy, compliance, disclosures) with opt-out footer.

Bulk_System_Service_Messages: High-volume system alerts (status, reminders) including standardized opt-out.

Platform_Appended_OptOut_Messages: Messages where STOP text is auto-added by SMS/WhatsApp gateways.

User_OptOut_Enabled_Service_Alerts: Service messages allowing users to stop future alerts via reply.





Personalized Calendar Events (Non-Promotional)

User-linked, intent-driven, or explicitly added to the calendar.

Personal_Meetings_Appointments: Doctor visits, office meetings, interviews, service appointments.

Personal_Deadlines: Work submissions, exams, applications, tax or document deadlines.

Personal_Travel_Plans: Flights, trains, hotel stays, vacations, business trips.

Personal_Financial_Reminders: Rent, EMIs, utility bills, subscription renewals.

Personal_Celebrations: Birthdays, weddings, anniversaries, family functions.

Personal_Exams_Evaluations: School exams, interviews, certification tests.

Personal_Ticketed_Events: Concerts, sports matches, conferences already booked.

Promotional Calendar Events

Discovery- or marketing-driven, not yet committed by the user.

Event_Promotions: Concerts, festivals, exhibitions, conferences being advertised.

Ticket_Sales_Alerts: Early-bird offers, last-minute deals, seat availability.

Holiday_Festival_Promos: New Year events, cultural festivals, public celebrations.

Learning_Event_Promotions: Webinars, workshops, bootcamps, online classes.

Venue_Show_Promotions: Club nights, theater shows, comedy events.

Travel_Event_Promos: Tour packages, cruise departures, group trips.

Brand_Sponsored_Events: Product launches, mall events, sponsored meetups.










import random
from torch.utils.data import Sampler

class LangDomainBalancedSampler(Sampler):
    """
    Balanced sampler that:
      - Creates 1 balanced batch per (language, domain)
      - Each batch contains 50% positives, 50% negatives
      - Performs oversampling when needed
      - Works with HF Trainer
    """

    def __init__(self, dataset, batch_size):
        assert batch_size % 2 == 0, "Batch size must be even"
        self.dataset = dataset
        self.batch_size = batch_size
        self.half = batch_size // 2

        # Build groups for (language, domain, label)
        self.groups = self._build_groups()
        self.group_keys = list(self.groups.keys())

    def _build_groups(self):
        groups = {}
        for idx, item in enumerate(self.dataset):
            key = (item["language"], item["domain"])
            if key not in groups:
                groups[key] = {0: [], 1: []}
            groups[key][item["label"]].append(idx)
        return groups

    def __iter__(self):
        keys = self.group_keys.copy()
        random.shuffle(keys)

        for key in keys:
            pos_list = self.groups[key][1]
            neg_list = self.groups[key][0]

            # Cannot create a balanced batch for this group
            if len(pos_list) == 0 or len(neg_list) == 0:
                continue

            # ---- Positive sampling ----
            pos_indices = (
                random.sample(pos_list, self.half)
                if len(pos_list) >= self.half
                else random.choices(pos_list, k=self.half)
            )

            # ---- Negative sampling ----
            neg_indices = (
                random.sample(neg_list, self.half)
                if len(neg_list) >= self.half
                else random.choices(neg_list, k=self.half)
            )

            batch = pos_indices + neg_indices
            random.shuffle(batch)

            for idx in batch:
                yield idx  # Dataloader groups sequential indices into batches

    def __len__(self):
        valid_group_count = sum(
            1
            for k in self.group_keys
            if len(self.groups[k][0]) > 0 and len(self.groups[k][1]) > 0
        )
        return valid_group_count * self.batch_size



from transformers import Trainer
from torch.utils.data import DataLoader

class BalancedTrainer(Trainer):
    def get_train_dataloader(self):
        sampler = LangDomainBalancedSampler(
            self.train_dataset,
            batch_size=self.args.per_device_train_batch_size
        )

        return DataLoader(
            self.train_dataset,
            batch_size=self.args.per_device_train_batch_size,
            sampler=sampler,
            collate_fn=self.data_collator
        )

    # Eval → do NOT use custom sampler
    # HF default behavior is correct


trainer = BalancedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

------------------------------------------------------

## class weights

from torch import nn
import torch
from transformers import AutoModel, SequenceClassifierOutput

class MiniLMWithHead(nn.Module):
    def __init__(self, model_name, num_labels=2, class_weights=None):
        super().__init__()

        self.encoder = AutoModel.from_pretrained(model_name)
        hidden_size = self.encoder.config.hidden_size

        # Freeze backbone if needed
        for p in self.encoder.parameters():
            p.requires_grad = False

        # classifier head
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_labels)
        )

        # register class weights
        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float)
        else:
            self.class_weights = None

        self.num_labels = num_labels
        self.loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=False,
            return_dict=True,
        )

        # CLS pooling
        cls = outputs.last_hidden_state[:, 0, :]   # [batch, hidden]

        logits = self.classifier(cls)

        loss = None
        if labels is not None:
            loss = self.loss_fn(logits, labels)

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
        )


from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import torch

labels = np.array(train_labels)
cls_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(labels),
    y=labels
)

model = MiniLMWithHead(
    model_name="nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large",
    num_labels=2,
    class_weights=cls_weights
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)
trainer.train()


---------------------------------------------
## classification head with transformer block

import torch
import torch.nn as nn
from transformers import AutoModel, SequenceClassifierOutput

class MiniLMClassifier(nn.Module):
    def __init__(self, model_name, num_labels=2):
        super().__init__()

        # --- Backbone ---
        self.encoder = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.encoder.config.hidden_size

        # Freeze MiniLM backbone
        for p in self.encoder.parameters():
            p.requires_grad = False

        # --- ALWAYS add 1 transformer block on top ---
        self.top_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=self.hidden_size,
                nhead=8,
                dim_feedforward=self.hidden_size * 4,
                batch_first=True
            ),
            num_layers=1
        )

        # --- Classifier head ---
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size * 4),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(self.hidden_size * 4, num_labels),
        )

        self.loss_fn = nn.CrossEntropyLoss()

    def mean_pooling(self, hidden_states, attention_mask):
        mask = attention_mask.unsqueeze(-1)                  # [B, L, 1]
        summed = (hidden_states * mask).sum(dim=1)           # [B, H]
        counts = mask.sum(dim=1)                             # [B, 1]
        return summed / counts

    def forward(self, input_ids, attention_mask, labels=None):
        # --- Get MiniLM outputs ---
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state            # [B, L, H]

        # --- Process through extra transformer block ---
        hidden_states = self.top_encoder(hidden_states)

        # --- Mean pooling ---
        pooled = self.mean_pooling(hidden_states, attention_mask)

        # --- Classifier ---
        logits = self.classifier(pooled)

        # --- Compute loss ---
        loss = None
        if labels is not None:
            loss = self.loss_fn(logits, labels)

        # --- HF output ---
        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


import pandas as pd

def sample_group(g):
    # keep all 'b'
    b_part = g[g['negtype'] == 'b']
    
    # sample 1 random 'a' if exists
    a_part = g[g['negtype'] == 'a'].sample(1, random_state=42) \
             if (g['negtype'] == 'a').any() else pd.DataFrame()
    
    return pd.concat([b_part, a_part], ignore_index=True)

df_out = df.groupby("text", group_keys=False).apply(sample_group)
